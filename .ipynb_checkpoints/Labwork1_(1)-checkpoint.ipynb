{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vm2Ex3wUtQ03"
   },
   "source": [
    "Run the following cell to ensure numba is installed (skip if you are sure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fpCCY92tQ09",
    "outputId": "aeb4ebbb-8772-4b58-a180-bc5e15182344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in c:\\users\\hieuq\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.58.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\hieuq\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from numba) (0.41.1)\n",
      "Requirement already satisfied: numpy<1.27,>=1.22 in c:\\users\\hieuq\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from numba) (1.24.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\hieuq\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgdhCzAxtQ1A"
   },
   "source": [
    "Run the following cell to verify that numba can use Cuda..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWR9QkEXtQ1B",
    "outputId": "6ed91c7c-c482-4e2b-d86d-4cb655dee521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is cuda available? no\r\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "from numba import cuda\n",
    "\n",
    "def bool_to_str(predicate: bool) -> str:\n",
    "    if predicate:\n",
    "        return 'yes'\n",
    "    return 'no'\n",
    "\n",
    "def is_cuda_available_str() -> str:\n",
    "    return bool_to_str(cuda.is_available())\n",
    "\n",
    "print(f'is cuda available? {is_cuda_available_str()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CSTJ2ldtQ1C"
   },
   "source": [
    "Another simple solution to detect and print the devices from [numba documentation](https://numba.readthedocs.io/en/stable/cuda-reference/host.html#device-management):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLf0s9U0tQ1D",
    "outputId": "8724e1f4-83eb-4c87-db57-dd7a11fe1676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'GeForce GTX 1050 Ti'                              [SUPPORTED]\n",
      "                      Compute Capability: 6.1\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-6d8e1ed7-4b24-5716-1782-904e5457d239\n",
      "                                Watchdog: Enabled\n",
      "                            Compute Mode: WDDM\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "from numba import cuda\n",
    "\n",
    "if not cuda.detect():\n",
    "    raise Exception(\"we do not have cuda :-(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-G9gnFktQ1E"
   },
   "source": [
    "Run the following cell for a short test of numba/cuda..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_SJ9x5yutQ1F",
    "outputId": "e262887b-e131-4ff1-aba3-afe440dcd862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it woks!\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "# This is a MAP example\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def increment_by_one(an_array):\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    ty = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    # Compute flattened index inside the array\n",
    "    pos = tx + ty * bw\n",
    "    if pos < an_array.size:  # Check array boundaries\n",
    "        an_array[pos] += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # build a big vector\n",
    "    h_array = np.arange(1<<16)\n",
    "\n",
    "    d_array = cuda.to_device(h_array)\n",
    "\n",
    "    threads_per_block = 32*8 # 8 warps, 256 threads per block\n",
    "    blocks_per_grid = (h_array.size + (threads_per_block - 1)) // threads_per_block\n",
    "    increment_by_one[blocks_per_grid, threads_per_block](d_array)\n",
    "\n",
    "    d_array.copy_to_host(h_array)\n",
    "\n",
    "    for i in range(len(h_array)):\n",
    "        assert h_array[i] == i+1, f'bad value at index {i}: {h_array[i]+1}'\n",
    "    print('it woks!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hHl5umvtQ1G"
   },
   "source": [
    "From here, it is clear that some code are easy to do:\n",
    "- binary transform\n",
    "- gather\n",
    "- scatter\n",
    "\n",
    "That's what you did during the first week... Now let us try some simple PRAM algorithms.\n",
    "\n",
    "The main problem with PRAM model is that it is too simple to be applied directly to GPU. Indeed a GPU is not a simple vector processor but a set of vector processor.\n",
    "\n",
    "You already saw that a Cuda Grid contains some Cuda Blocks, and each Cuda Block contains some threads.\n",
    "The threads are organized into **warps**.\n",
    "\n",
    "A warp is the logical Cuda view of a vector processor, with 32 elementary processors. Hence, threads are working in vector mode by group of 32...\n",
    "\n",
    "So, how to program PRAM algorithms in Cuda? Good question, thanks.\n",
    "\n",
    "Today we are working into a single block (we will see how to use different blocks concurrently later) with some warps. Remember that the maximum number of threads you can use into a block is limited to 1024. The warp is a group of 32 threads, so the maximum number of warps is 32!\n",
    "\n",
    "Now, how to simulate PRAM into a block? For that purpose we need to synchronize the warps, to avoid race condition onto data. The simple solution to synchronizer all the threads into a block is to use the instruction `cuda.syncthreads()`.\n",
    "\n",
    "**Warning**: this instruction needs to be used by **all** the threads of the block. It is like a barrier, and it opens when all the threads reach it only...\n",
    "\n",
    "To avoid high latency and synchronisation problem, notice that all the data should be first loaded into block shared memory... This kind of memory should be allocated using the `numba.cuda.array(shape, dtype)` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiavOro6tQ1H"
   },
   "source": [
    "## First exercise\n",
    "The objective here is to write a Cuda algorithm that calculates the maximum of 32 values into a single block. Your implementation should be added at line 21!\n",
    "\n",
    "32 threads means a single warp, so no synchronisation is needed here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vgYfLxwtQ1I",
    "outputId": "d8fa35b0-63dd-4c81-aebb-bd7641d9f316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel computation time is 138.60231018066406 ms\n",
      "h_maximum:  [1046026]\n",
      "this is the array: [ 512657  604975  983948  420230  484146  798490  776581  954040  836843\n",
      "  681237  802253  228372 1008105  972985  473930  839256  412401  103240\n",
      "   65931  977942  908702 1046026  621256  584821  706574  820633  465137\n",
      "  402768   31030  843382  489568  327985] with length: 32\n",
      "this is: 512657 and the maximum is 1046026\n",
      "this is: 604975 and the maximum is 1046026\n",
      "this is: 983948 and the maximum is 1046026\n",
      "this is: 420230 and the maximum is 1046026\n",
      "this is: 484146 and the maximum is 1046026\n",
      "this is: 798490 and the maximum is 1046026\n",
      "this is: 776581 and the maximum is 1046026\n",
      "this is: 954040 and the maximum is 1046026\n",
      "this is: 836843 and the maximum is 1046026\n",
      "this is: 681237 and the maximum is 1046026\n",
      "this is: 802253 and the maximum is 1046026\n",
      "this is: 228372 and the maximum is 1046026\n",
      "this is: 1008105 and the maximum is 1046026\n",
      "this is: 972985 and the maximum is 1046026\n",
      "this is: 473930 and the maximum is 1046026\n",
      "this is: 839256 and the maximum is 1046026\n",
      "this is: 412401 and the maximum is 1046026\n",
      "this is: 103240 and the maximum is 1046026\n",
      "this is: 65931 and the maximum is 1046026\n",
      "this is: 977942 and the maximum is 1046026\n",
      "this is: 908702 and the maximum is 1046026\n",
      "this is: 1046026 and the maximum is 1046026\n",
      "this is: 621256 and the maximum is 1046026\n",
      "this is: 584821 and the maximum is 1046026\n",
      "this is: 706574 and the maximum is 1046026\n",
      "this is: 820633 and the maximum is 1046026\n",
      "this is: 465137 and the maximum is 1046026\n",
      "this is: 402768 and the maximum is 1046026\n",
      "this is: 31030 and the maximum is 1046026\n",
      "this is: 843382 and the maximum is 1046026\n",
      "this is: 489568 and the maximum is 1046026\n",
      "this is: 327985 and the maximum is 1046026\n",
      "maximum seems to work\n"
     ]
    }
   ],
   "source": [
    "# so this is the ecrew | check the algorithms in the slide 21 of day 2.1\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "from numba.np.numpy_support import from_dtype\n",
    "\n",
    "\n",
    "class CudaMaximum:\n",
    "    _kernels_cache = {}\n",
    "\n",
    "    def __init__(self: CudaMaximum) -> None:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory(np_type):\n",
    "        \"\"\"Factory of kernels for the maximum problem...\n",
    "\n",
    "        This function returns a Cuda Kernel that does the maximum of some data using a single block.\"\"\"\n",
    "\n",
    "        def kernel(d_input, d_maximum) -> None:\n",
    "            tid = cuda.threadIdx.x\n",
    "            shared = cuda.shared.array(shape=32, dtype=d_input.dtype)\n",
    "            # do it here\n",
    "            #tid is the id of input\n",
    "            n = d_input.size\n",
    "            # if tid < n:\n",
    "            #   shared[tid] = d_input[tid]\n",
    "            # else:\n",
    "            #   shared[tid] = d_input[0]\n",
    "            shared[tid] = d_input[tid] if tid<n else d_input[0]\n",
    "            # because it not a for so use tid not using i\n",
    "\n",
    "            # shared[tid] = (tid < n) ? d_input[tid] : d_input[0]\n",
    "\n",
    "            j = 1 # not related to the tid\n",
    "\n",
    "            while j < n:\n",
    "              if tid + j < n: # because in the slide it is already 1 to n so don't need equal here\n",
    "                temp = shared[tid+j]\n",
    "                if shared[tid] < temp:\n",
    "                    shared[tid] = temp\n",
    "              j = j*2\n",
    "            if tid == 0:\n",
    "              d_maximum[0] = shared[0]\n",
    "\n",
    "            # for i in range(len(d_input)):\n",
    "            #   shared[i] = d_input[i]\n",
    "\n",
    "            # j = 1\n",
    "            # while j < len(d_input):\n",
    "            #   for i in range(len(d_input)):\n",
    "            #     if i+j <= len(d_input):\n",
    "            #       shared[i] = max(shared[i], shared[i+j])\n",
    "            #   j = j*2\n",
    "\n",
    "            # d_maximum = shared\n",
    "\n",
    "        return cuda.jit(kernel)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compile(dtype):\n",
    "\n",
    "        key = dtype\n",
    "        if key not in CudaMaximum._kernels_cache:\n",
    "            CudaMaximum._kernels_cache[key] = CudaMaximum._gpu_kernel_factory(from_dtype(dtype))\n",
    "\n",
    "        return CudaMaximum._kernels_cache[key]\n",
    "\n",
    "    def __call__(self, buffer, maximum, stream=cuda.default_stream()):\n",
    "        \"\"\"Computes the maximum.\n",
    "\n",
    "        :param buffer: A device array, containing the data.\n",
    "        :param stream: Optional CUDA stream in which to perform the calculations.\n",
    "                    If no stream is specified, the default stream of 0 is used.\n",
    "        :return: ``None``\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure 1d array\n",
    "        if buffer.ndim != 1:\n",
    "            raise TypeError(\"only support 1D array\")\n",
    "\n",
    "        # ensure size > 0\n",
    "        if buffer.size < 1:\n",
    "            raise ValueError(\"array's length is 0\")\n",
    "\n",
    "        # ensure size == 1\n",
    "        if maximum.size != 1:\n",
    "            raise ValueError(\"array's length is not zero\")\n",
    "\n",
    "        # ensure size <= 32\n",
    "        if buffer.size > 32:\n",
    "            raise ValueError(\"array's length is too big\")\n",
    "\n",
    "        kernel = self._compile(buffer.dtype)\n",
    "\n",
    "        # Perform the maximum on the GPU\n",
    "        nb_threads = buffer.size\n",
    "        nb_blocks = 1\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        stop_event = cuda.event(True)\n",
    "\n",
    "        start_event.record(stream=stream)\n",
    "        kernel[nb_blocks, nb_threads, stream](buffer, maximum)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "        ct = cuda.event_elapsed_time(start_event, stop_event)\n",
    "        print(f\"kernel computation time is {ct} ms\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def check(maximum, array, msg):\n",
    "        error = 0\n",
    "        print(f'this is the array: {array} with length: {len(array)}')\n",
    "        for x in array:\n",
    "            print(f'this is: {x} and the maximum is {maximum}')\n",
    "            if x > maximum:\n",
    "                error = error + 1\n",
    "        if error > 0:\n",
    "            print(f\"{msg} does not work: {error} errors generated\")\n",
    "        else:\n",
    "            print(f\"{msg} seems to work\")\n",
    "\n",
    "\n",
    "    def test(h_buffer):\n",
    "        d_buffer = cuda.to_device(h_buffer)\n",
    "        d_maximum = cuda.device_array(shape=1, dtype=d_buffer.dtype)\n",
    "\n",
    "        maxer = CudaMaximum()\n",
    "        maxer(d_buffer, d_maximum)\n",
    "\n",
    "        h_maximum = d_maximum.copy_to_host()\n",
    "        print('h_maximum: ', h_maximum)\n",
    "        check(h_maximum[0], h_buffer, \"maximum\")\n",
    "\n",
    "\n",
    "    core.config.CUDA_LOW_OCCUPANCY_WARNINGS = False\n",
    "    buffer = np.random.randint(low=0, high=1 << 20, size=32, dtype=np.int32)\n",
    "    test(buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8LD-w9gtQ1K"
   },
   "source": [
    "## Second exercise\n",
    "The objective here is to write a Cuda algorithm that calculates the maximum of 1024 values into a single block. Your implementation should be added at line 21!\n",
    "\n",
    "Take care: use threads' synchronization to simulate the PRAM algorithm with multiple warps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2Tvf6EDtQ1L",
    "outputId": "47012b68-fb13-4649-b397-e6e51a17c9cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel computation time is 132.4877471923828 ms\n",
      "maximum seems to work\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "from numba.np.numpy_support import from_dtype\n",
    "\n",
    "\n",
    "class CudaMaximum:\n",
    "    _kernels_cache = {}\n",
    "\n",
    "    def __init__(self: CudaMaximum) -> None:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory(np_type, nb_threads):\n",
    "        \"\"\"Factory of kernels for the maximum problem...\n",
    "\n",
    "        This function returns a Cuda Kernel that does the maximum of some data using a single block.\"\"\"\n",
    "\n",
    "        def kernel(d_input, d_maximum) -> None:\n",
    "            tid = cuda.threadIdx.x\n",
    "            shared = cuda.shared.array(shape=nb_threads, dtype=d_input.dtype)\n",
    "\n",
    "            n = d_input.size\n",
    "            shared[tid] = d_input[tid] if tid<n else d_input[0]\n",
    "            # cuda.syncthreads() # wait for all to finish\n",
    "            j = 1\n",
    "            # khi dùng if và while sẽ xảy ra race condition => dùng cude sync để tránh điều đó\n",
    "            while j < n:\n",
    "              cuda.syncthreads() # assume that we have more than one block | dòng này sẽ khiến cái bên trên chạy rồi nên bỏ cái bên trên\n",
    "              # if tid + j < n:\n",
    "              #   temp = shared[tid+j]\n",
    "                  # it should be here, but here is inside the if so break the if\n",
    "              #   if shared[tid] < temp:\n",
    "              #       shared[tid] = temp # be sure\n",
    "\n",
    "              if tid + j < n:\n",
    "                temp = shared[tid+j]\n",
    "              cuda.syncthreads() # why here?\n",
    "              if tid + j < n:\n",
    "                if shared[tid] < temp:\n",
    "                    shared[tid] = temp # be sure\n",
    "\n",
    "              # cuda.syncthreads()\n",
    "\n",
    "              j = j*2\n",
    "            if tid == 0:\n",
    "              d_maximum[0] = shared[0]\n",
    "            # bây giờ đã chia thành nhiều block nên max[0] tạo thành arr[block_size]\n",
    "\n",
    "        return cuda.jit(kernel)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compile(dtype, nb_threads):\n",
    "\n",
    "        key = dtype, nb_threads\n",
    "        if key not in CudaMaximum._kernels_cache:\n",
    "            CudaMaximum._kernels_cache[key] = CudaMaximum._gpu_kernel_factory(from_dtype(dtype), nb_threads)\n",
    "\n",
    "        return CudaMaximum._kernels_cache[key]\n",
    "\n",
    "    def __call__(self, buffer, maximum, stream=cuda.default_stream()):\n",
    "        \"\"\"Computes the maximum.\n",
    "\n",
    "        :param buffer: A device array, containing the data.\n",
    "        :param stream: Optional CUDA stream in which to perform the calculations.\n",
    "                    If no stream is specified, the default stream of 0 is used.\n",
    "        :return: ``None``\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure 1d array\n",
    "        if buffer.ndim != 1:\n",
    "            raise TypeError(\"only support 1D array\")\n",
    "\n",
    "        # ensure size > 0\n",
    "        if buffer.size < 1:\n",
    "            raise ValueError(\"array's length is 0\")\n",
    "\n",
    "        # ensure size == 1\n",
    "        if maximum.size != 1:\n",
    "            raise ValueError(\"array's length is not zero\")\n",
    "\n",
    "        # ensure size < 1024+1\n",
    "        if buffer.size > 1024:\n",
    "            raise ValueError(\"array's length is too big\")\n",
    "\n",
    "        # Perform the maximum on the GPU\n",
    "        nb_threads = 1024\n",
    "        nb_blocks = 1\n",
    "\n",
    "        kernel = self._compile(buffer.dtype, nb_threads)\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        stop_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "        kernel[nb_blocks, nb_threads, stream](buffer, maximum)\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "        ct = cuda.event_elapsed_time(start_event, stop_event)\n",
    "        print(f\"kernel computation time is {ct} ms\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def check(maximum, array, msg):\n",
    "        error = 0\n",
    "        for x in array:\n",
    "            if x > maximum:\n",
    "                error = error + 1\n",
    "        if error > 0:\n",
    "            print(f\"{msg} does not work: {error} errors generated\")\n",
    "        else:\n",
    "            print(f\"{msg} seems to work\")\n",
    "\n",
    "\n",
    "    def test(h_buffer):\n",
    "        d_buffer = cuda.to_device(h_buffer)\n",
    "        d_maximum = cuda.device_array(shape=1, dtype=d_buffer.dtype)\n",
    "\n",
    "        maxer = CudaMaximum()\n",
    "        maxer(d_buffer, d_maximum)\n",
    "\n",
    "        h_maximum = d_maximum.copy_to_host()\n",
    "\n",
    "        check(h_maximum[0], h_buffer, \"maximum\")\n",
    "\n",
    "\n",
    "    core.config.CUDA_LOW_OCCUPANCY_WARNINGS = False\n",
    "    test(np.random.randint(low=0, high=1 << 20, size=1024, dtype=np.int32))\n",
    "\n",
    "# The placement of cuda.syncthreads() in the code is important to ensure the correctness and efficiency of the parallel algorithm. The cuda.syncthreads() function acts as a barrier that synchronizes all the threads in a block before proceeding to the next step. This prevents race conditions and data hazards when multiple threads access the same shared memory location.\n",
    "\n",
    "# In this code, the cuda.syncthreads() function is used twice in the while loop:\n",
    "\n",
    "# The first time, it is used after loading the data from the global memory to the shared memory. This ensures that all the threads have finished reading their input values before performing any comparisons.\n",
    "# The second time, it is used after reading a temporary value from the shared memory. This ensures that all the threads have finished updating their shared memory values before comparing them with the temporary value.\n",
    "# The cuda.syncthreads() function is not needed before or after the while loop, because there is no data dependency between different iterations of the loop. The final result is stored in the shared memory location of thread 0, which is then copied to the global memory location of d_maximum.\n",
    "#========================\n",
    "# In CUDA programming, there are different types of memory that can be used for different purposes. In this code, global memory and shared memory are used.\n",
    "\n",
    "# Global Memory: This is the main memory space on the GPU device and it can be accessed by all threads as well as the host (CPU). It has a large capacity but relatively high latency. In this code, d_input and d_maximum are stored in global memory. They are passed as arguments to the kernel function and can be accessed by all threads in the kernel. d_input is the input data array and d_maximum is used to store the final result of the computation.\n",
    "\n",
    "# Shared Memory: This is a small, user-managed cache that is shared by all threads in a block. It has much lower latency than global memory, but it’s also much smaller. In this code, shared is an array stored in shared memory. It’s used as a temporary storage for the input data during the computation. Each thread reads its input value from global memory and stores it in shared memory for faster access during the computation.\n",
    "\n",
    "# The use of shared memory in this code helps to speed up the computation by reducing the number of global memory accesses, which are slower than shared memory accesses. However, because shared memory is limited, this code only works for input data arrays of size 32 or less.\n",
    "#============\n",
    "# Yes, you can place a cuda.syncthreads() call above the line j = j*2. This would ensure that all threads in the block have completed their computations for the current iteration of the loop before proceeding to the next iteration.\n",
    "\n",
    "# However, in your specific code, it’s not necessary because there’s already a cuda.syncthreads() call at the beginning of the loop. This already ensures that all threads have completed their computations from the previous iteration before proceeding.\n",
    "\n",
    "# Remember, excessive use of cuda.syncthreads() can lead to performance degradation due to unnecessary synchronization overhead. It’s best to use it judiciously only when necessary to prevent race conditions. 😊\n",
    "#=========\n",
    "# Yes, you can place a cuda.syncthreads() call above the line j = 1. This would ensure that all threads in the block have completed their computations and have loaded their data into the shared memory before proceeding to the loop.\n",
    "\n",
    "# However, in your specific code, it’s not necessary because there’s no computation or memory operation happening between the initialization of shared array and j = 1. The cuda.syncthreads() function is typically used when you want to make sure all threads have reached a certain point in the code, especially when they are sharing data that might lead to race conditions. In this case, since no such condition exists, adding cuda.syncthreads() would not have any effect and might add unnecessary synchronization overhead.\n",
    "#==============\n",
    "# A race condition is a situation in concurrent programming where two or more threads are able to access shared data and they try to change it at the same time. As a result, the values of variables may be unpredictable and vary depending on the timings of context switches of the processes.\n",
    "\n",
    "# For example, consider a simple banking system with an operation that allows two account holders to withdraw money from their joint account. If both account holders withdraw money from the account at the same time, one of the following scenarios could occur:\n",
    "\n",
    "# The system checks if there is enough money in the account for both withdrawals before making any deductions.\n",
    "# The system deducts the first withdrawal, then checks if there is enough money left for the second withdrawal.\n",
    "# The system deducts the second withdrawal before checking if there is enough money left for the first withdrawal.\n",
    "# In the first scenario, both withdrawals may go through even if there isn’t enough money in the account to cover both, leading to an overdraft. In the second and third scenarios, one of the withdrawals may fail even if there was enough money to cover both before any withdrawal was made. These are examples of race conditions.\n",
    "\n",
    "# In multithreaded programs, race conditions can be prevented by using locks, semaphores, or other synchronization techniques to ensure that only one thread can access the shared data at a time. In your CUDA code, cuda.syncthreads() is a synchronization function that ensures all threads in a block have reached the same point in the code before proceeding, which can help prevent race conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIU9glNetQ1N"
   },
   "source": [
    "## Third exercise\n",
    "You may have notice that the first two exercises works with 1024 values only.\n",
    "If you remove the line:\n",
    "```python\n",
    "    core.config.CUDA_LOW_OCCUPANCY_WARNINGS = False\n",
    "```\n",
    "Then you will see a warning from Cuda saying that the GPU is under-utilized.\n",
    "Indeed, we ran a single block, onto a single SMP (*Streaming Multi-Processor*), while there is plenty of SMP (from 4 to tens)...\n",
    "To overcome this problem, the solution is quite simple but not obvious.\n",
    "\n",
    "First, you have to launch multiple block, but no more than 256 to avoid registers' pressure (shared memory is simulated using registers, and so too big shared memory implied low number of registers per thread, and so reduced efficiency...).\n",
    "\n",
    "Then, you obtain one maximum value per block, and this value should be saved into a specific isolated memory location: so you need an array of maximums of size equals to the number of blocks...\n",
    "\n",
    "At last, well you have something like 256 maximum values... Hum, it is where the first exercise is useful ;-)\n",
    "\n",
    "Modify you implementation to work with many values (something like 1 millions, at least)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYPrNSxxtQ1N",
    "outputId": "b414f73f-780a-4ea9-f7ca-d80fa6f9b21e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "launch 131072 for 33554432 threads\n",
      "launch 512 for 131072 threads\n",
      "launch 2 for 512 threads\n",
      "kernel computation time is 507.3227233886719 ms\n",
      "Maximum is 1048575\n",
      "maximum seems to work\n"
     ]
    }
   ],
   "source": [
    "# read at global and write at local\n",
    "# 1 kernel for single block and one for multiple block\n",
    "# shared doesn't exist on cpu\n",
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda, core\n",
    "from numba.np.numpy_support import from_dtype\n",
    "\n",
    "\n",
    "class CudaMaximum:\n",
    "    _kernels_1_cache = {}\n",
    "    _kernels_2_cache = {}\n",
    "\n",
    "    def __init__(self: CudaMaximum) -> None:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_1(np_type, nb_threads):\n",
    "        \"\"\"Factory of kernels for the maximum problem...\n",
    "\n",
    "        This function returns a Cuda Kernel that does the maximum of some data using a multiple block.\"\"\"\n",
    "\n",
    "        def kernel(d_input, d_maximum) -> None:\n",
    "            gtid = cuda.grid(1) # global index\n",
    "            ltid = cuda.threadIdx.x # local index\n",
    "            bdim = cuda.blockDim.x\n",
    "\n",
    "            shared = cuda.shared.array(shape=nb_threads, dtype=d_input.dtype)\n",
    "            # do it here\n",
    "            n = d_input.size\n",
    "            shared[ltid] = d_input[gtid] if gtid < n else d_input[0]\n",
    "\n",
    "            j = 1\n",
    "\n",
    "            while j < bdim: # j less than the size of the block\n",
    "              cuda.syncthreads()\n",
    "\n",
    "              if ltid + j < bdim: # gtid + j less than size of array\n",
    "                temp = shared[ltid+j]\n",
    "              cuda.syncthreads()\n",
    "              if ltid + j < bdim: # can remove this: gtid + j < n?\n",
    "                if shared[ltid] < temp:\n",
    "                    shared[ltid] = temp\n",
    "              j = j*2\n",
    "\n",
    "            if ltid == 0:\n",
    "              d_maximum[cuda.blockIdx.x] = shared[0]\n",
    "\n",
    "        return cuda.jit(kernel)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compile_step1(dtype, nb_threads):\n",
    "\n",
    "        key = dtype, nb_threads\n",
    "        if key not in CudaMaximum._kernels_1_cache:\n",
    "            CudaMaximum._kernels_1_cache[key] = CudaMaximum._gpu_kernel_factory_1(from_dtype(dtype), nb_threads)\n",
    "\n",
    "        return CudaMaximum._kernels_1_cache[key]\n",
    "\n",
    "    @staticmethod\n",
    "    def _gpu_kernel_factory_2(np_type, nb_threads):\n",
    "        \"\"\"Factory of kernels for the maximum problem...\n",
    "\n",
    "        This function returns a Cuda Kernel that does the maximum of some data using a single block.\"\"\"\n",
    "\n",
    "        def kernel(d_input, d_maximum) -> None:\n",
    "            tid = cuda.threadIdx.x\n",
    "            shared = cuda.shared.array(shape=nb_threads, dtype=d_input.dtype)\n",
    "            # do it here\n",
    "            n = d_input.size\n",
    "            shared[tid] = d_input[tid] if tid<n else d_input[0]\n",
    "            # cuda.syncthreads() # wait for all to finish\n",
    "            j = 1\n",
    "            # khi dùng if và while sẽ xảy ra race condition => dùng cude sync để tránh điều đó\n",
    "            while j < n:\n",
    "              cuda.syncthreads() # assume that we have more than one block | dòng này sẽ khiến cái bên trên chạy rồi nên bỏ cái bên trên\n",
    "              # if tid + j < n:\n",
    "              #   temp = shared[tid+j]\n",
    "                  # it should be here, but here is inside the if so break the if\n",
    "              #   if shared[tid] < temp:\n",
    "              #       shared[tid] = temp # be sure\n",
    "\n",
    "              if tid + j < n:\n",
    "                temp = shared[tid+j]\n",
    "              cuda.syncthreads() # why here?\n",
    "              if tid + j < n:\n",
    "                if shared[tid] < temp:\n",
    "                    shared[tid] = temp # be sure\n",
    "\n",
    "              # cuda.syncthreads()\n",
    "\n",
    "              j = j*2\n",
    "            if tid == 0:\n",
    "              d_maximum[0] = shared[0]\n",
    "\n",
    "        return cuda.jit(kernel)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compile_step2(dtype, nb_threads):\n",
    "\n",
    "        key = dtype\n",
    "        if key not in CudaMaximum._kernels_2_cache:\n",
    "            CudaMaximum._kernels_2_cache[key] = CudaMaximum._gpu_kernel_factory_2(from_dtype(dtype), nb_threads)\n",
    "\n",
    "        return CudaMaximum._kernels_2_cache[key]\n",
    "\n",
    "    def __call__(self, buffer, maximum, stream=0):\n",
    "        \"\"\"Computes the maximum.\n",
    "\n",
    "        :param buffer: A device array, containing the data.\n",
    "        :param stream: Optional CUDA stream in which to perform the calculations.\n",
    "                       If no stream is specified, the default stream of 0 is used.\n",
    "        :return: ``None``\n",
    "        \"\"\"\n",
    "\n",
    "        # ensure 1d array\n",
    "        if buffer.ndim != 1:\n",
    "            raise TypeError(\"only support 1D array\")\n",
    "\n",
    "        # ensure size > 0\n",
    "        if buffer.size < 1:\n",
    "            raise ValueError(\"array's length is 0\")\n",
    "\n",
    "        # ensure size == 1\n",
    "        if maximum.size != 1:\n",
    "            raise ValueError(\"array's length is not zero\")\n",
    "\n",
    "        # Perform the maximum per block on the GPU\n",
    "        nb_threads = 256\n",
    "        kernel_1 = self._compile_step1(buffer.dtype, nb_threads)\n",
    "\n",
    "        start_event = cuda.event(True)\n",
    "        stop_event = cuda.event(True)\n",
    "        start_event.record(stream=stream)\n",
    "\n",
    "        while buffer.size > 256:\n",
    "            nb_blocks = (buffer.size + nb_threads - 1) // nb_threads\n",
    "            print(f\"launch {nb_blocks} for {nb_threads * nb_blocks} threads\")\n",
    "\n",
    "            temp = cuda.device_array(shape=nb_blocks, dtype=buffer.dtype)\n",
    "\n",
    "            kernel_1[nb_blocks, nb_threads, stream](buffer, temp)\n",
    "\n",
    "            cuda.synchronize()\n",
    "\n",
    "            buffer = temp\n",
    "\n",
    "        # second step...\n",
    "        kernel_2 = self._compile_step2(buffer.dtype, buffer.size)\n",
    "        kernel_2[1, buffer.size, stream](buffer, maximum)\n",
    "\n",
    "        stop_event.record(stream=stream)\n",
    "        stop_event.synchronize()\n",
    "        ct = cuda.event_elapsed_time(start_event, stop_event)\n",
    "        print(f\"kernel computation time is {ct} ms\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def check(maximum, array, msg):\n",
    "        error = 0\n",
    "        for x in array:\n",
    "            if x > maximum:\n",
    "                error = error + 1\n",
    "        if error > 0:\n",
    "            print(f\"{msg} does not work: {error} errors generated\")\n",
    "        else:\n",
    "            print(f\"{msg} seems to work\")\n",
    "\n",
    "\n",
    "    def test(h_buffer):\n",
    "        d_buffer = cuda.to_device(h_buffer)\n",
    "        d_maximum = cuda.device_array(shape=1, dtype=d_buffer.dtype)\n",
    "\n",
    "        maxer = CudaMaximum()\n",
    "        maxer(d_buffer, d_maximum)\n",
    "\n",
    "        h_maximum = d_maximum.copy_to_host()\n",
    "        print(f\"Maximum is {h_maximum[0]}\")\n",
    "\n",
    "        check(h_maximum[0], h_buffer, \"maximum\")\n",
    "\n",
    "\n",
    "    core.config.CUDA_LOW_OCCUPANCY_WARNINGS = False\n",
    "    buffer = np.random.randint(low=0, high=1 << 20, size=1 << 25, dtype=np.int32)\n",
    "    test(buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDBxN30d_Z6E",
    "outputId": "e71f06f1-ab94-418b-837c-f06d9316be11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "x = 10\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVQkomJ0q91a",
    "outputId": "5ab852a0-c504-46fc-92f9-4b8eba17bc82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "x = 10.0\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5pGTB9-_jsi",
    "outputId": "0ab2d240-3971-45dd-a807-a555cb010c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "x = 'Hello'\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoCBVfDW_llB",
    "outputId": "f2877e27-4134-4485-abff-4f182764e991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "x = [10, 11, 12]\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8DB58xL_oZv"
   },
   "outputs": [],
   "source": [
    "def cube(x): return x*x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkGx3EB7ADNb",
    "outputId": "39b2e3b5-2e8b-4fc2-973c-5c8abcbc887f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdPRvHV-AFjb"
   },
   "outputs": [],
   "source": [
    "def double_print(name):\n",
    "  print(name)\n",
    "  print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpCF60VUATUo",
    "outputId": "3162c62f-1af3-41b7-a2ae-c46c7d680047"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello students\n",
      "Hello students\n"
     ]
    }
   ],
   "source": [
    "double_print('Hello students')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7EDVo50AV_C"
   },
   "outputs": [],
   "source": [
    "def check_negative(x):\n",
    "  if x < 0:\n",
    "    print('This is a negative number')\n",
    "  else:\n",
    "    print('This is not a negative number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4o14fekjAub3",
    "outputId": "5fb5d97c-acd0-4749-ad68-414ee822adc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a negative number\n"
     ]
    }
   ],
   "source": [
    "check_negative(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8Mzv2wGAwyu"
   },
   "outputs": [],
   "source": [
    "def sum(a, b):\n",
    "  return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F7ddGX2jA96O",
    "outputId": "d327f9c1-05c2-4cac-d3d5-1b3439f730f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YclCTAIEA_2v"
   },
   "outputs": [],
   "source": [
    "def name_and_age(name, age):\n",
    "  if age < 18:\n",
    "    print('Under 18 years old')\n",
    "  print('My name is', name, 'I am', age, 'years old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdld7-J8D4Lx",
    "outputId": "5a8d11d3-0916-49df-9eec-fef5c147e875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is  Hieu I am  18 years old\n"
     ]
    }
   ],
   "source": [
    "name_and_age('Hieu', 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wER_Uq08D7lg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b4d75ac280b6c7c3aa43866cb82dc88915409b55fec83a093dd0284cb58708e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
